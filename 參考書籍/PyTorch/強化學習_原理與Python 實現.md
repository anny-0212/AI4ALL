#
```
強化學習：原理與Python 實現
肖智清
機械工業出版社
2019/8/1 0:00:00
https://github.com/ZhiqingXiao/rl-book
```

```
第1 章　初識強化學習 1
1.1　強化[0學0]習及其關鍵元素 1
1.2　強化[0學0]習的應用 3
1.3　智慧體/ 環境介面 4
1.4　強化[0學0]習的分類 6
1.4.1　按任務分類 6
1.4.2　按算[0法0]分類 7
1.5　如何[0學0]習強化[0學0]習 8
1.5.1　[0學0]習路線 9
1.5.2　[0學0]習資源 9
1.6　 案例：基於Gym 庫的智慧體/
環境交互 9
1.6.1　安裝Gym 庫 10
1.6.2　使用Gym 庫 10
1.6.3　小車上山 12
1.7　本章小結 14
[0第0]2 章　Markov 決策過程 16
2.1　Markov 決策過程模型 16
2.1.1　 離散時間Markov 決策
過程 16
2.1.2　環境與動力 18
2.1.3　智能體與策略 19
2.1.4　獎勵、回報與價值函數 19
C O N T E N T S
目　　錄
2.2　Bellman 期望方程 21
2.3　[z1u1i][0優0]策略及其性質 25
2.3.1　 [z1u1i][0優0]策略與[z1u1i][0優0]價值
函數 25
2.3.2　Bellman [z1u1i][0優0]方程 25
2.3.3　 用Bellman [z1u1i][0優0]方程求解
[z1u1i][0優0]策略 29
2.4　案例：懸崖尋路 31
2.4.1　實驗環境使用 31
2.4.2　求解Bellman 期望方程 32
2.4.3　求解Bellman [z1u1i][0優0]方程 33
2.5　本章小結 35
[0第0]3 章　有模型數值反覆運算 37
3.1　度量空間與壓縮映[身寸] 37
3.1.1　度量空間及其完備性 37
3.1.2　 壓縮映[身寸]與Bellman
運算元 38
3.1.3　Banach 不動點定理 39
3.2　有模型策略反覆運算 40
3.2.1　策略[0評0]估 40
3.2.2　策略改進 42
3.2.3　策略反覆運算 44
3.3　有模型價值反覆運算 45
3.4　動態規劃 46
3.4.1　從動態規劃看反覆運算算[0法0] 46
VI
3.4.2　非同步動態規劃 47
3.5　案例：冰[mian]滑行 47
3.5.1　實驗環境使用 48
3.5.2　有模型策略反覆運算求解 49
3.5.3　有模型價值反覆運算求解 51
3.6　本章小結 52
[0第0]4 章　回合更[親斤]價值反覆運算 54
4.1　同策回合更[親斤] 54
4.1.1　同策回合更[親斤]策略[0評0]估 54
4.1.2　 帶起始探索的同策回合
更[親斤] 58
4.1.3　 基於柔性策略的同策回合
更[親斤] 60
4.2　異策回合更[親斤] 62
4.2.1　重要性採樣 62
4.2.2　異策回合更[親斤]策略[0評0]估 64
4.2.3　 異策回合更[親斤][z1u1i][0優0]策略
求解 65
4.3　案例：21 點遊戲 66
4.3.1　實驗環境使用 66
4.3.2　同策策略[0評0]估 67
4.3.3　同策[z1u1i][0優0]策略求解 70
4.3.4　異策策略[0評0]估 72
4.3.5　異策[z1u1i][0優0]策略求解 73
4.4　本章小結 74
[0第0]5 章　時序差分價值反覆運算 76
5.1　同策時序差分更[親斤] 76
5.1.1　時序差分更[親斤]策略[0評0]估 78
5.1.2　SARSA 算[0法0] 81
5.1.3　期望SARSA 算[0法0] 83
5.2　異策時序差分更[親斤] 85
5.2.1　 基於重要性採樣的異策
算[0法0] 85
5.2.2　Q [0學0]習 86
5.2.3　[0雙0]重Q [0學0]習 87
5.3　資格跡 89
5.3.1　λ 回報 89
5.3.2　TD(λ) 90
5.4　案例：計程車調度 92
5.4.1　實驗環境使用 93
5.4.2　同策時序差分[0學0]習調度 94
5.4.3　異策時序差分[0學0]習調度 97
5.4.4　資格跡[0學0]習調度 99
5.5　本章小結 100
[0第0]6 章　函數近似方[0法0] 101
6.1　函數近似原理 101
6.1.1　隨[1機1]梯度下降 101
6.1.2　半梯度下降 103
6.1.3　帶資格跡的半梯度下降 105
6.2　線性近似 107
6.2.1　 精確查閱資料表與線性近似的
關係 107
6.2.2　線性[z1u1i]小二乘策略[0評0]估 107
6.2.3　 線性[z1u1i]小二乘[z1u1i][0優0]策略
求解 109
6.3　函數近似的收斂性 109
6.4　深度Q [0學0]習 110
6.4.1　[糸巠]驗重播 111
6.4.2　 帶目標網路的深度Q
[0學0]習 112
6.4.3　[0雙0]重深度Q 網路 114
6.4.4　對偶深度Q 網路 114
6.5　案例：小車上山 115
6.5.1　實驗環境使用 116
6.5.2　 用線性近似求解[z1u1i][0優0]
策略 117
VII　
6.5.3　 用深度Q [0學0]習求解[z1u1i][0優0]
策略 120
6.6　本章小結 123
[0第0]7 章　回合更[親斤]策略梯度方[0法0] 125
7.1　策略梯度算[0法0]的原理 125
7.1.1　函數近似與動作偏[女子] 125
7.1.2　策略梯度定理 126
7.2　同策回合更[親斤]策略梯度算[0法0] 128
7.2.1　簡單的策略梯度算[0法0] 128
7.2.2　 帶基線的簡單策略梯度
算[0法0] 129
7.3　異策回合更[親斤]策略梯度算[0法0] 131
7.4　 策略梯度更[親斤]和[0極0][0大0]似然估計的
關係 132
7.5　案例：車杆平衡 132
7.5.1　 同策策略梯度算[0法0]求解
[z1u1i][0優0]策略 133
7.5.2　 異策策略梯度算[0法0]求解
[z1u1i][0優0]策略 135
7.6　本章小結 137
[0第0]8 章　執行者/ [0評0]論者方[0法0] 139
8.1　同策執行者/ [0評0]論者算[0法0] 139
8.1.1　 動作價值執行者/ [0評0]論者
算[0法0] 140
8.1.2　 [0優0]勢執行者/ [0評0]論者
算[0法0] 141
8.1.3　 帶資格跡的執行者/
[0評0]論者算[0法0] 143
8.2　基於代理[0優0]勢的同策算[0法0] 143
8.2.1　代理[0優0]勢 144
8.2.2　鄰近策略[0優0]化 145
8.3　信任域算[0法0] 146
8.3.1　KL 散度 146
8.3.2　信任域 147
8.3.3　自然策略梯度算[0法0] 148
8.3.4　信任域策略[0優0]化 151
8.3.5　 Kronecker 因數信任域
執行者/ [0評0]論者算[0法0] 152
8.4　 重要性採樣異策執行者/ [0評0]論者
算[0法0] 153
8.4.1　基本的異策算[0法0] 154
8.4.2　帶[糸巠]驗重播的異策算[0法0] 154
8.5　柔性執行者/ [0評0]論者算[0法0] 157
8.5.1　熵 157
8.5.2　獎勵工程和帶熵的獎勵 158
8.5.3　 柔性執行者/ [0評0]論者的
網路設計 159
8.6　案例：[0雙0]節倒立擺 161
8.6.1　 同策執行者/ [0評0]論者算[0法0]
求解[z1u1i][0優0]策略 162
8.6.2　 異策執行者/ [0評0]論者算[0法0]
求解[z1u1i][0優0]策略 168
8.7　本章小結 170
[0第0]9 章　 連續動作空間的確定性
策略 172
9.1　同策確定性算[0法0] 172
9.1.1　 策略梯度定理的確定性
版本 172
9.1.2　 基本的同策確定性執行者/
[0評0]論者算[0法0] 174
9.2　異策確定性算[0法0] 176
9.2.1　 基本的異策確定性執行者/
[0評0]論者算[0法0] 177
9.2.2　 深度確定性策略梯度
算[0法0] 177
9.2.3　 [0雙0]重延遲深度確定性
策略梯度算[0法0] 178
VIII
9.3　案例：倒立擺的控制 180
9.3.1　 用深度確定性策略梯度算[0法0]求解 181
9.3.2　 用[0雙0]重延遲深度確定性
算[0法0]求解 184
9.4　本章小結 187

[0第0]10 章　綜合案例：電動遊戲 188
10.1　Atari 遊戲環境 188
10.1.1　Gym 庫的完整安裝 188
10.1.2　遊戲環境使用 190
10.2　基於深度Q [0學0]習的遊戲AI 191
10.2.1　算[0法0]設計 192
10.2.2　智慧體的實現 193
10.2.3　智能體的訓練和測試 197
10.3　本章小結 198

[0第0]11 章　綜合案例：棋盤遊戲 200
11.1　[0雙0]人確定性棋盤遊戲 200
11.1.1　五子棋和井字棋 200
11.1.2　黑白棋 201
11.1.3　圍棋 202
11.2　AlphaZero 算[0法0] 203
11.2.1　回合更[親斤]樹搜索 203
11.2.2　深度殘差網路 206
11.2.3　自我對弈 208
11.2.4　算[0法0]流程 210
11.3　棋盤遊戲環境boardgame2 210
11.3.1　 為Gym 庫擴展自訂環境 211
11.3.2　boardgame2 設計 211
11.3.3　Gym 環境介面的實現 214
11.3.4　樹搜索介面的實現 216
11.4　AlphaZero 算[0法0]實現 218
11.4.1　智慧體類的實現 218
11.4.2　自我對弈的實現 223
11.4.3　訓練智能體 224
11.5　本章小結 225

第12 章　綜合案例：自動駕駛 226
12.1　AirSim 開發環境使用 226
12.1.1　安裝和運行AirSim 226
12.1.2　用Python 訪問AirSim 228
12.2　基於強化[0學0]習的自動駕駛 229
12.2.1　 為自動駕駛設計強化學習環境 230
12.2.2　智慧體設計和實現 235
12.2.3　智能體的訓練和測試 237
12.3　本章小結 239

```
