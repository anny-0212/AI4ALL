#
```
神經網絡與 PyTorch 實戰 神经网络与PyTorch实战
肖智清
機械工業
2018-08-14

https://github.com/ZhiqingXiao/pytorch-book
```

```
第1章初識神經網絡1 
1.1例說神經網絡1 
1.1.1從圍棋和AlphaGo說起1 
1.1.2人的神經系統3 
1.1.3人工神經元3 
1.1.4人工神經網絡5 
1.1 .5神經網絡的設計和權重的學習7 
1.2神經網絡與人工智能等概念的關係7 
1.2.1人工智能和數據挖掘7 
1.2.2機器學習和模式識別9 
1.2.3人工神經網絡和深度學習11 
1.2.4各概念之間的聯繫11 
1.3本章小結12 

第2章初識PyTorch13 
2.1人工神經網絡庫PyTorch13 
2.1.1 PyTorch是什麼13 
2.1.2編寫PyTorch程序14 
2.2例說PyTorch14 
2.2.1迷你AlphaGo介紹15 
2.2.2迷你AlphaGo的完整實現16 
2.3 PyTorch學習路線19 
2.4本章小結20 

第3章使用PyTorch進行科學計算21 
3.1初識張量21 
3.1.1張量的數學定義21
3.1.2 PyTorch裡的張量22 
3.2構造torch.Tensor類實例24 
3.2.1構造含有特定數據的張量24 
3.2.2構造特定大小的張量25 
3.2.3構造等比數列和等差數列張量26 
3.2.4構造隨機張量26 
3.3組織張量的元素28 
3.3.1重排張量元素28 
3.3.2選取部分張量元素29 
3.3.3張量的擴展和拼接31 
3.4張量的科學計算32 
3.4.1有理運算和廣播語義32 
3.4.2逐元素運算33 
3.4.3張量點積和Einstein求和35 
3.4.4統計函數38 
3.4.5比較和邏輯運算39 
3.5例子：用蒙特卡洛算法計算圓周率40 
3.5.1隨機計算與蒙特卡洛算法40 
3.5.2蒙特卡洛算法求解圓周率的實現41 
3.6本章小結42 

第4章求解優化問題43 
4.1梯度及其計算43 
4.1.1梯度的定義43 
4.1.2梯度的性質和計算45 
4.1.3使用PyTorch計算梯度數值45 
4.2優化算法與torch.optim包46 
4.2.1梯度下降算法46
4.2.2梯度下降算法的缺陷和解決方案48 
4.2.3各種優化算法50 
4.3例子：Himmelblau函數的優化55 
4.3.1 Himmelblau函數及可視化55 
4.3.2求解Himmelblau的最小值57 
4.3.3求解Himmelblau的局部極大值59 
4.4本章小結59 

第5章線性回歸60 
5.1一元線性回歸60 
5.1.1最小二乘法60 
5.1.2正規方程法62 
5.2多元線性回歸63 
5.3其他損失情況下的線性回歸63 
5.3 .1 MSE損失、損失和平滑損失64 
5.3.2 torch.nn子包與損失類65 
5.3.3使用優化器求解線性回歸66 
5.3.4數據的歸一化68 
5.4例子：世界人口的線性回歸70 
5.4.1從維基百科頁面獲取世界人口數據70 
5.4.2對世界人口做最小二乘法線性回歸71 
5.4.3用優化算法實現最小二乘回歸72 
5.5本章小結74 

第6章線性判決與邏輯回歸75 
6.1線性判決與互熵損失75 
6.1.1判定問題與準確率75 
6.1.2線性判決76 
6.1.3極大似然和互熵損失77
6.2邏輯回歸78 
6.2.1 expit()函數和logit()函數78 
6.2.2用優化器實現邏輯回歸80 
6.2.3 Newton-Raphson方法81 
6.3多項邏輯回歸82 
6.4例子：數字圖像的識別84 
6.4 .1使用torchvision讀取MNIST數據集84 
6.4.2利用多項邏輯回歸識別MNIST數據86 
6.5例子：股票成交量預測88 
6.5.1股票數據的讀取和可視化88 
6.5.2成交量變化方向預測89 
6.6本章小結91 

第7章全連接神經網絡92 
7.1前饋神經網絡92 
7.1.1前饋神經網絡的定義92 
7.1.2使用torch.nn.Sequential類搭建前饋神經網絡93 
7.1.3權重的確定與反向傳播94 
7.2全連接層和全連接神經網絡95 
7.3非線性激活96 
7.3.1逐元素激活97 
7.3.2非逐元素激活101 
7.4網絡結構的選擇102 
7.4.1欠擬合和過擬合102 
7.4.2訓練集、驗證集和測試集103 
7.5例子：基於全連接網絡的非線性回歸105 
7.5.1數據的生成和數據集分割105
7.5.2確定網絡結構並訓練網絡106 
7.5.3測試性能108 
7.6本章小結109
 
第8章卷積神經網絡110 
8.1卷積層110 
8.1.1序列的互相關和卷積110 
8.1.2一維張量的互相關114 
8.1.3一維張量的轉置卷積117 
8.1.4高維張量的互相關和轉置卷積121 
8.1.5 torch.nn包裡的捲積層121 
8.2池化層、視覺層和補全層123 
8.2.1張量的池化124 
8.2.2張量的反池化125 
8.2.3 torch.nn包裡的池化層126 
8.2.4張量的上採樣128 
8.2.5 torch.nn包裡的視覺層130 
8.2.6張量的補全運算131 
8.2.7 torch.nn包裡的補全層131 
8.3例子：MNIST圖片分類的改進132 
8.3.1搭建卷積神經網絡133 
8.3.2卷積神經網絡的訓練和測試135 
8.4本章小結137 

第9章循環神經網絡138 
9.1神經網絡的循環結構138 
9.1.1單向單層循環結構138 
9.1.2多層循環結構139 
9.1.3雙向循環結構140
9.2循環神經網絡中的循環單元141 
9.2.1基本循環神經元141 
9.2.2長短期記憶單元141 
9.2.3門控循環單元144 
9.3循環神經網絡的實現145 
9.3.1 torch.nn子包中的循環單元類145 
9.3.2 torch.nn子包中的循環神經網絡類146 
9.4例子：人均GDP的預測147 
9.4.1使用pandas-datareader讀取世界銀行數據庫147 
9.4.2搭建LSTM預測模型148 
9.4. 3網絡的訓練和使用149 
9.5本章小結151 

第10章生成對抗網絡152 
10.1生成對抗
```
