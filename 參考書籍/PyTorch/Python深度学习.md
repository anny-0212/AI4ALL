#
```
Python深度学习：基于PyTorch
作者: 吴茂贵 郁明敏 杨本法 李涛 张粤磊
机械工业出版社
```

```
前言
第一部分 PyTorch基礎
第1章 Numpy基礎2
1.1 生成Numpy陣列3
1.1.1 從已有資料中創建陣列3
1.1.2 利用random模組生成陣列4
1.1.3 創建特定形狀的多維陣列5
1.1.4 利用arange、linspace函數生成陣列6
1.2 獲取元素7
1.3 Numpy的算數運算9
1.3.1 對應元素相乘9
1.3.2 點積運算10
1.4 陣列變形11
1.4.1 更改陣列的形狀11
1.4.2 合併陣列14
1.5 批量處理16
1.6 通用函數17
1.7 廣播機制19
1.8 小結20


第2章 PyTorch基礎21
2.1 為何選擇PyTorch？21
2.2 安裝配置22
2.2.1 安裝CPU版PyTorch22
2.2.2 安裝GPU版PyTorch24
2.3 Jupyter Notebook環境配置26
2.4 Numpy與Tensor28
2.4.1 Tensor概述28
2.4.2 創建Tensor28
2.4.3 修改Tensor形狀30
2.4.4 索引操作31
2.4.5 廣播機制32
2.4.6 逐元素操作32
2.4.7 歸併操作33
2.4.8 比較操作34
2.4.9 矩陣操作35
2.4.10 PyTorch與Numpy比較35
2.5 Tensor與Autograd36
2.5.1 自動求導要點36
2.5.2 計算圖37
2.5.3 標量反向傳播38
2.5.4 非標量反向傳播39
2.6 使用Numpy實現機器學習41
2.7 使用Tensor及Antograd實現機器學習44
2.8 使用TensorFlow架構46
2.9 小結48
第

3章 PyTorch神經網路工具箱49
3.1 神經網路核心元件49
3.2 實現神經網路實例50
3.2.1 背景說明51
3.2.2 準備數據52
3.2.3 視覺化來源資料53
3.2.4 構建模型53
3.2.5 訓練模型54
3.3 如何構建神經網路？56
3.3.1 構建網路層56
3.3.2 前向傳播57
3.3.3 反向傳播57
3.3.4 訓練模型58
3.4 神經網路工具箱nn58
3.4.1 nn.Module58
3.4.2 nn.functional58
3.5 優化器59
3.6 動態修改學習率參數60
3.7 優化器比較60
3.8 小結62


第4章 PyTorch資料處理工具箱63
4.1 資料處理工具箱概述63
4.2 utils.data簡介64

4.3 torchvision簡介66
4.3.1 transforms67
4.3.2 ImageFolder67

4.4 視覺化工具69
4.4.1 tensorboardX簡介69
4.4.2 用tensorboardX視覺化神經網路71
4.4.3 用tensorboardX視覺化損失值72
4.4.4 用tensorboardX視覺化特徵圖73
4.5 本章小結74


第二部分 深度學習基礎
第5章 機器學習基礎76
5.1 機器學習的基本任務76
5.1.1 監督學習77
5.1.2 無監督學習77
5.1.3 半監督學習78
5.1.4 強化學習78
5.2 機器學習一般流程78
5.2.1 明確目標79
5.2.2 收集資料79
5.2.3 資料探索與預處理79
5.2.4 選擇模型及損失函數80
5.2.5 評估及優化模型81
5.3 過擬合與欠擬合81
5.3.1 權重正則化82
5.3.2 Dropout正則化83
5.3.3 批量正則化86
5.3.4 權重初始化88
5.4 選擇合適啟動函數89
5.5 選擇合適的損失函數90
5.6 選擇合適優化器92
5.6.1 傳統梯度優化的不足93
5.6.2 動量演算法94
5.6.3 AdaGrad演算法96
5.6.4 RMSProp演算法97
5.6.5 Adam演算法98
5.7 GPU加速99
5.7.1 單GPU加速100
5.7.2 多GPU加速101
5.7.3 使用GPU注意事項104
5.8 本章小結104


第6章 視覺處理基礎105
6.1 卷積神經網路簡介105
6.2 卷積層107
6.2.1 卷積核108
6.2.2 步幅109
6.2.3 填充111
6.2.4 多通道上的卷積111
6.2.5 啟動函數113
6.2.6 卷積函數113
6.2.7 轉置卷積114
6.3 池化層115
6.3.1 局部池化116
6.3.2 全域池化117
6.4 現代經典網路119
6.4.1 LeNet-5模型119
6.4.2 AlexNet模型120
6.4.3 VGG模型121
6.4.4 GoogleNet模型122
6.4.5 ResNet模型123
6.4.6 膠囊網路簡介124
6.5 PyTorch實現CIFAR-10多分類125
6.5.1 資料集說明125
6.5.2 載入數據125
6.5.3 構建網路127
6.5.4 訓練模型128
6.5.5 測試模型129
6.5.6 採用全域平均池化130
6.5.7 像Keras一樣顯示各層參數131
6.6 模型集成提升性能133
6.6.1 使用模型134
6.6.2 集成方法134
6.6.3 集成效果135
6.7 使用現代經典模型提升性能136
6.8 本章小結137


第7章 自然語言處理基礎138
7.1 迴圈神經網路基本結構138
7.2 前向傳播與隨時間反向傳播140
7.3 迴圈神經網路變種143
7.3.1 LSTM144
7.3.2 GRU145
7.3.3 Bi-RNN146
7.4 迴圈神經網路的PyTorch實現146
7.4.1 RNN實現147
7.4.2 LSTM實現149
7.4.3 GRU實現151
7.5 文本資料處理152
7.6 詞嵌入153
7.6.1 Word2Vec原理154
7.6.2 CBOW模型155
7.6.3 Skip-Gram模型155
7.7 PyTorch實現詞性判別156
7.7.1 詞性判別主要步驟156
7.7.2 數據預處理157
7.7.3 構建網路157
7.7.4 訓練網路158
7.7.5 測試模型160
7.8 用LSTM預測股票行情160
7.8.1 導入數據160
7.8.2 數據概覽161
7.8.3 預處理數據162
7.8.4 定義模型163
7.8.5 訓練模型163
7.8.6 測試模型164
7.9 迴圈神經網路應用場景165
7.10 小結166


第8章 生成式深度學習167
8.1 用變分自編碼器生成圖像167
8.1.1 自編碼器168
8.1.2 變分自編碼器168
8.1.3 用變分自編碼器生成圖像169
8.2 GAN簡介173
8.2.1 GAN架構173
8.2.2 GAN的損失函數174
8.3 用GAN生成圖像175
8.3.1 判別器175
8.3.2 生成器175
8.3.3 訓練模型175
8.3.4 視覺化結果177
8.4 VAE與GAN的優缺點178
8.5 ConditionGAN179
8.5.1 CGAN的架構179
8.5.2 CGAN生成器180
8.5.3 CGAN判別器180
8.5.4 CGAN損失函數181
8.5.5 CGAN視覺化181
8.5.6 查看指定標籤的數據182
8.5.7 視覺化損失值182
8.6 DCGAN183
8.7 提升GAN訓練效果的一些技巧184
8.8 小結185



第三部分 深度學習實踐
第9章 人臉檢測與識別188
9.1 人臉識別一般流程188
9.2 人臉檢測189
9.2.1 目標檢測189
9.2.2 人臉定位191
9.2.3 人臉對齊191
9.2.4 MTCNN演算法192
9.3 特徵提取193
9.4 人臉識別198
9.4.1 人臉識別主要原理198
9.4.2 人臉識別發展198
9.5 PyTorch實現人臉檢測與識別199
9.5.1 驗證檢測代碼199
9.5.2 檢測圖像200
9.5.3 檢測後進行預處理200
9.5.4 查看經檢測後的圖像201
9.5.5 人臉識別202
9.6 小結202


第10章 遷移學習實例203
10.1 遷移學習簡介203
10.2 特徵提取204
10.2.1 PyTorch提供的預處理模組205
10.2.2 特徵提取實例206
10.3 資料增強209
10.3.1 按比例縮放209
10.3.2 裁剪210
10.3.3 翻轉210
10.3.4 改變顏色211
10.3.5 組合多種增強方法211
10.4 微調實例212
10.4.1 數據預處理212
10.4.2 載入預訓練模型213
10.4.3 修改分類器213
10.4.4 選擇損失函數及優化器213
10.4.5 訓練及驗證模型214
10.5 清除圖像中的霧霾214
10.6 小結217


第11章 神經網路機器翻譯實例218
11.1 Encoder-Decoder模型原理218
11.2 注意力框架220

11.3 PyTorch實現注意力Decoder224
11.3.1 構建Encoder224
11.3.2 構建簡單Decoder225
11.3.3 構建注意力Decoder226

11.4 用注意力機制實現中英文互譯227
11.4.1 導入需要的模組228
11.4.2 數據預處理228
11.4.3 構建模型231
11.4.4 訓練模型234
11.4.5 隨機採樣，對模型進行測試235
11.4.6 視覺化注意力236
11.5 小結237


第12章 實戰生成式模型238
12.1 DeepDream模型238
12.1.1 Deep Dream原理238
12.1.2 DeepDream演算法流程239
12.1.3 用PyTorch實現Deep Dream240


12.2 風格遷移243
12.2.1 內容損失244
12.2.2 風格損失245
12.2.3 用PyTorch實現神經網路風格遷移247

12.3 PyTorch實現圖像修復252
12.3.1 網路結構252
12.3.2 損失函數252
12.3.3 圖像修復實例253
12.4 PyTorch實現DiscoGAN255
12.4.1 DiscoGAN架構256
12.4.2 損失函數258
12.4.3 DiscoGAN實現258
12.4.4 用PyTorch實現從邊框生成鞋子260
12.5 小結262


第13章 Caffe2模型遷移實例263
13.1 Caffe2簡介263
13.2 Caffe如何升級到Caffe2264
13.3 PyTorch如何遷移到Caffe2265
13.4 小結268


第14章 AI新方向：對抗攻擊269
14.1 對抗攻擊簡介269
14.1.1 白盒攻擊與黑盒攻擊270
14.1.2 無目標攻擊與有目標攻擊270
14.2 常見對抗樣本生成方式271
14.2.1 快速梯度符號法271
14.2.2 快速梯度演算法271
14.3 PyTorch實現對抗攻擊272
14.3.1 實現無目標攻擊272
14.3.2 實現有目標攻擊274
14.4 對抗攻擊和防禦措施276
14.4.1 對抗攻擊276
14.4.2 常見防禦方法分類276
14.5 總結277


第15章 強化學習278
15.1 強化學習簡介278
15.2 Q-Learning原理281
15.2.1 Q-Learning主要流程281
15.2.2 Q函數282
15.2.3 貪婪策略283
15.3 用PyTorch實現Q-Learning283
15.3.1 定義Q-Learing主函數283
15.3.2 執行Q-Learing284
15.4 SARSA演算法285
15.4.1 SARSA演算法主要步驟285
15.4.2 用PyTorch實現SARSA演算法286
15.5 小結287

第16章 深度強化學習288
16.1 DQN演算法原理288
16.1.1 Q-Learning方法的局限性289
16.1.2 用DL處理RL需要解決的問題289
16.1.3 用DQN解決方法289
16.1.4 定義損失函數290
16.1.5 DQN的經驗重播機制290
16.1.6 目標網路290
16.1.7 網路模型291
16.1.8 DQN演算法291
16.2 用PyTorch實現DQN演算法292
16.3 小結295
附錄A PyTorch0.4版本變更296
附錄B AI在各行業的最新應用301

```
